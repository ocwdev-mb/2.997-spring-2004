---
content_type: page
description: ''
learning_resource_types:
- Readings
ocw_type: CourseSection
title: Readings
uid: 6897a3ff-dccb-36d9-9430-7f489f653321
---

Bertsekas = Bertsekas, Dimitri P. _Dynamic Programming and Optimal Control_. 2 vols. Belmont, MA: Athena Scientific, 2007. ISBN: 9781886529083.

Bertsekas and Tsitsiklis = Bertsekas, Dimitri P., and John N. Tsitsiklis. _Neuro-Dynamic Programming_. Belmont, MA: Athena Scientific, 1996. ISBN: 9781886529106.

{{< tableopen >}}
{{< theadopen >}}
{{< tropen >}}
{{< thopen >}}
LECÂ #
{{< thclose >}}
{{< thopen >}}
TOPICS
{{< thclose >}}
{{< thopen >}}
READINGS
{{< thclose >}}

{{< trclose >}}

{{< theadclose >}}
{{< tropen >}}
{{< tdopen >}}
1
{{< tdclose >}}
{{< tdopen >}}
Markov Decision Processes  
  
Finite-Horizon Problems: Backwards Induction  
  
Discounted-Cost Problems: Cost-to-Go Function, Bellman's Equation
{{< tdclose >}}
{{< tdopen >}}
Bertsekas Vol. 1, Chapter 1.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
2
{{< tdclose >}}
{{< tdopen >}}
Value Iteration  
  
Existence and Uniqueness of Bellman's Equation Solution  
  
Gauss-Seidel Value Iteration
{{< tdclose >}}
{{< tdopen >}}
Bertsekas Vol. 2, Chapter 1.  
  
Bertsekas and Tsitsiklis, Chapter 2.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
3
{{< tdclose >}}
{{< tdopen >}}
Optimality of Policies derived from the Cost-to-Go Function  
  
Policy Iteration  
  
Asynchronous Policy Iteration
{{< tdclose >}}
{{< tdopen >}}
Bertsekas Vol. 2, Chapter 1.  
  
Bertsekas and Tsitsiklis, Chapter 2.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
4
{{< tdclose >}}
{{< tdopen >}}
Average-Cost Problems  
  
Relationship with Discounted-Cost Problems  
  
Bellman's Equation  
  
Blackwell Optimality
{{< tdclose >}}
{{< tdopen >}}
Bertsekas Vol. 2, Chapter 4.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
5
{{< tdclose >}}
{{< tdopen >}}
Average-Cost Problems  
  
Computational Methods
{{< tdclose >}}
{{< tdopen >}}
Bertsekas Vol. 2, Chapter 4.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
6
{{< tdclose >}}
{{< tdopen >}}
Application of Value Iteration to Optimization of Multiclass Queueing Networks  
  
Introduction to Simulation-based Methods Real-Time Value Iteration
{{< tdclose >}}
{{< tdopen >}}
Chen, R. R., and S. P. Meyn. "[Value Iteration and Optimization of Multiclass Queueing Networks](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.8423)."_Queueing Systems_ 32 (1999): 65-97.  
  
Bertsekas and Tsitsiklis, Chapter 5.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
7
{{< tdclose >}}
{{< tdopen >}}
Q-Learning  
  
Stochastic Approximations
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapters 4 and 5.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
8
{{< tdclose >}}
{{< tdopen >}}
Stochastic Approximations: Lyapunov Function Analysis  
  
The ODE Method  
  
Convergence of Q-Learning
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapters 4 and 5.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
9
{{< tdclose >}}
{{< tdopen >}}
Exploration versus Exploitation: The Complexity of Reinforcement Learning
{{< tdclose >}}
{{< tdopen >}}
Kearns, M. , and S. Singh. "[Near-Optional Reinforcement Learning in Polynomial Time](http://www.cis.upenn.edu/~mkearns/papers/reinforcement.pdf)." _Machine Learning_ 49, no. 2 (Nov 2002): 209-232.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
10
{{< tdclose >}}
{{< tdopen >}}
Introduction to Value Function Approximation  
  
Curse of Dimensionality  
  
Approximation Architectures
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
11
{{< tdclose >}}
{{< tdopen >}}
Model Selection and Complexity
{{< tdclose >}}
{{< tdopen >}}
Hastie, Tibshirani, and Friedmann. Chapter 7 in _The Elements of Statistical Learning_. New York: Springer, 2003. ISBN: 9780387952840.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
12
{{< tdclose >}}
{{< tdopen >}}
Introduction to Value Function Approximation Algorithms  
  
Performance Bounds
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
13
{{< tdclose >}}
{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
14
{{< tdclose >}}
{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation (cont.)
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.  
  
de Farias, D. P., and B. Van Roy. "[On the Existence of Fixed Points for Approximate Value Iteration and Temporal-Difference Learning](http://dx.doi.org/10.1023/A:1004641123405)."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
15
{{< tdclose >}}
{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation (cont.)  
  
Optimal Stopping Problems  
  
General Control Problems
{{< tdclose >}}
{{< tdopen >}}
Bertsekas and Tsitsiklis, Chapter 6.  
  
de Farias, D. P., and B. Van Roy. "[On the Existence of Fixed Points for Approximate Value Iteration and Temporal-Difference Learning](http://dx.doi.org/10.1023/A:1004641123405)."  
  
Bertsekas, Borkar, and Nedic. "[Improved temporal Difference Methods with Linear Function Approximation](http://onlinelibrary.wiley.com/doi/10.1002/9780470544785.ch9/summary)."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
16
{{< tdclose >}}
{{< tdopen >}}
Approximate Linear Programming
{{< tdclose >}}
{{< tdopen >}}
de Farias, D. P., and B. Van Roy. "[The Linear Programming Approach to Approximate Dynamic Programming](http://www.mit.edu/~pucci/discountedLP.pdf)."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
17
{{< tdclose >}}
{{< tdopen >}}
Approximate Linear Programming (cont.)
{{< tdclose >}}
{{< tdopen >}}
de Farias, D. P., and B. Van Roy. "[The Linear Programming Approach to Approximate Dynamic Programming](http://www.mit.edu/~pucci/discountedLP.pdf)."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
18
{{< tdclose >}}
{{< tdopen >}}
Efficient Solutions for Approximate Linear Programming
{{< tdclose >}}
{{< tdopen >}}
de Farias D. P., and B. Van Roy. "[On Constraint Sampling in the Linear Programming Approach to Approximate Dynamic Programming](http://www.mit.edu/~pucci/sampling.pdf)."  
  
Calafiori, and Campi. "[Uncertain Convex Programs: Randomized Solutions and Confidence Levels](http://academic.research.microsoft.com/Publication/1744417/uncertain-convex-programs-randomized-solutions-and-confidence-levels)."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
19
{{< tdclose >}}
{{< tdopen >}}
Efficient Solutions for Approximate Linear Programming: Factored MDPs
{{< tdclose >}}
{{< tdopen >}}
Guestrin, et al. "[Efficient Solution Algorithms for Factored MDPs](http://www-2.cs.cmu.edu/afs/cs/project/jair/pub/volume19/guestrin03a.pdf)."  
  
Schuurmans, and Patrascu. "[Direct Value Approximation for Factored MDPs](http://citeseer.ist.psu.edu/schuurmans01direct.html)."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
20
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods
{{< tdclose >}}
{{< tdopen >}}
Marbach, and Tsitsiklis. "Simulation-Based Optimization of Markov Reward Processes." ([PDF](http://www.mit.edu/~jnt/Papers/J083-01-mar-MDP.pdf))
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
21
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods (cont.)
{{< tdclose >}}
{{< tdopen >}}
Baxter, and Bartlett. "[Infinite-Horizon Policy-Gradient Estimation](http://www-2.cs.cmu.edu/afs/cs/project/jair/pub/volume15/baxter01a.pdf)."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
22
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods for POMDPs  
  
Application: Call Admission Control  
  
Actor-Critic Methods
{{< tdclose >}}
{{< tdopen >}}
Baxter, and Bartlett. "[Infinite-Horizon Policy-Gradient Estimation](http://www-2.cs.cmu.edu/afs/cs/project/jair/pub/volume15/baxter01a.pdf)."  
  
Baxter, and Bartlett. "[Experiments with Infinite-Horizon Policy-Gradient Estimation](http://www-2.cs.cmu.edu/afs/cs/project/jair/pub/volume15/baxter01b.pdf)."  
  
Konda, and Tsitsiklis. "Actor-Critic Algorithms." ([PDF](http://www.mit.edu/~jnt/Papers/J094-03-kon-actors.pdf))
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
23
{{< tdclose >}}
{{< tdopen >}}
Guest Lecture: Prof. Nick Roy  
  
Approximate POMDP Compression
{{< tdclose >}}
{{< tdopen >}}
Roy, and Gordon. "[Exponential Family PCA for Belief Compression in POMDPs](http://web.mit.edu/nickroy/www/papers/nips02.pdf)."
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
24
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods: PEGASUS  
  
Application: Helicopter Control
{{< tdclose >}}
{{< tdopen >}}
Ng, and Jordan. "[PEGASUS: A policy search method for large MDPs and POMDPs](http://www.robotics.stanford.edu/~ang/papers/uai00-pegasus.pdf)."  
  
Ng, et al. "[Autonomous Helicopter Flight via Reinforcement Learning](http://books.nips.cc/papers/files/nips16/NIPS2003_CN07.pdf)."
{{< tdclose >}}

{{< trclose >}}

{{< tableclose >}}

Complementary Reading
---------------------

Even-Dar, and Mansour. "[Learning Rates for Q-Learning](http://dl.acm.org/citation.cfm?id=1005333).'' _Journal of Machine Learning Research_ 5 (2003): 1-25.

Barron. "Universal approximation bounds for superpositions of a sigmoidal function." _IEEE Transactions on Information Theory_ 39 (1993): 930-944.

Tesauro. "[Temporal-Difference Learning and TD-Gammon](https://dx.doi.org/10.1145/203330.203343)'' _Communications of the ACM_ 38, no. 3 (1995).