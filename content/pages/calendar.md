---
content_type: page
description: ''
learning_resource_types: []
ocw_type: CourseSection
title: Calendar
uid: 4d2b58df-f19e-28d4-e2f1-ef7d996632ea
---

{{< tableopen >}}
{{< theadopen >}}
{{< tropen >}}
{{< thopen >}}
LECÂ #
{{< thclose >}}
{{< thopen >}}
TOPICS
{{< thclose >}}
{{< thopen >}}
KEY DATES
{{< thclose >}}

{{< trclose >}}

{{< theadclose >}}
{{< tropen >}}
{{< tdopen >}}
1
{{< tdclose >}}
{{< tdopen >}}
Markov Decision Processes  
  
Finite-Horizon Problems: Backwards Induction  
  
Discounted-Cost Problems: Cost-to-Go Function, Bellman's Equation
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
2
{{< tdclose >}}
{{< tdopen >}}
Value Iteration  
  
Existence and Uniqueness of Bellman's Equation Solution  
  
Gauss-Seidel Value Iteration
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
3
{{< tdclose >}}
{{< tdopen >}}
Optimality of Policies derived from the Cost-to-go Function  
  
Policy Iteration  
  
Asynchronous Policy Iteration
{{< tdclose >}}
{{< tdopen >}}
Problem set 1 out
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
4
{{< tdclose >}}
{{< tdopen >}}
Average-Cost Problems  
  
Relationship with Discounted-Cost Problems  
  
Bellman's Equation  
  
Blackwell Optimality
{{< tdclose >}}
{{< tdopen >}}
Problem set 1 due
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
5
{{< tdclose >}}
{{< tdopen >}}
Average-Cost Problems  
  
Computational Methods
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
6
{{< tdclose >}}
{{< tdopen >}}
Application of Value Iteration to Optimization of Multiclass Queueing Networks  
  
Introduction to Simulation-based Methods Real-Time Value Iteration
{{< tdclose >}}
{{< tdopen >}}
Problem set 2 out
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
7
{{< tdclose >}}
{{< tdopen >}}
Q-Learning  
  
Stochastic Approximations
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
8
{{< tdclose >}}
{{< tdopen >}}
Stochastic Approximations: Lyapunov Function Analysis  
  
The ODE Method  
  
Convergence of Q-Learning
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
9
{{< tdclose >}}
{{< tdopen >}}
Exploration versus Exploitation: The Complexity of Reinforcement Learning
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
10
{{< tdclose >}}
{{< tdopen >}}
Introduction to Value Function Approximation  
  
Curse of Dimensionality  
  
Approximation Architectures
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
11
{{< tdclose >}}
{{< tdopen >}}
Model Selection and Complexity
{{< tdclose >}}
{{< tdopen >}}
Problem set 3 out
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
12
{{< tdclose >}}
{{< tdopen >}}
Introduction to Value Function Approximation Algorithms  
  
Performance Bounds
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
13
{{< tdclose >}}
{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
14
{{< tdclose >}}
{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation (cont.)
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
15
{{< tdclose >}}
{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation (cont.)  
  
Optimal Stopping Problems  
  
General Control Problems
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
16
{{< tdclose >}}
{{< tdopen >}}
Approximate Linear Programming
{{< tdclose >}}
{{< tdopen >}}
Problem set 4 out
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
17
{{< tdclose >}}
{{< tdopen >}}
Approximate Linear Programming (cont.)
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
18
{{< tdclose >}}
{{< tdopen >}}
Efficient Solutions for Approximate Linear Programming
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
19
{{< tdclose >}}
{{< tdopen >}}
Efficient Solutions for Approximate Linear Programming: Factored MDPs
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
20
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods
{{< tdclose >}}
{{< tdopen >}}
Problem set 5 out
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
21
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods (cont.)
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
22
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods for POMDPs  
  
Application: Call Admission Control  
  
Actor-Critic Methods
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
23
{{< tdclose >}}
{{< tdopen >}}
Guest Lecture: Prof. Nick Roy  
  
Approximate POMDP Compression
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
24
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods: PEGASUS  
  
Application: Helicopter Control
{{< tdclose >}}
{{< tdopen >}}

{{< tdclose >}}

{{< trclose >}}

{{< tableclose >}}