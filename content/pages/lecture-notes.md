---
content_type: page
description: ''
learning_resource_types:
- Lecture Notes
ocw_type: CourseSection
title: Lecture Notes
uid: 621c1e57-c07f-f26c-a822-25ef44476992
---

{{< tableopen >}}
{{< theadopen >}}
{{< tropen >}}
{{< thopen >}}
LEC #
{{< thclose >}}
{{< thopen >}}
TOPICS
{{< thclose >}}
{{< thopen >}}
LECTURE NOTES
{{< thclose >}}

{{< trclose >}}

{{< theadclose >}}
{{< tropen >}}
{{< tdopen >}}
1
{{< tdclose >}}
{{< tdopen >}}
Markov Decision Processes  
  
Finite-Horizon Problems: Backwards Induction  
  
Discounted-Cost Problems: Cost-to-Go Function, Bellman's Equation
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link 77727e6f-132a-fde1-0dbb-50d12613f518 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
2
{{< tdclose >}}
{{< tdopen >}}
Value Iteration  
  
Existence and Uniqueness of Bellman's Equation Solution  
  
Gauss-Seidel Value Iteration
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link e8711c96-c560-b5b5-3764-6b95131fe3e5 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
3
{{< tdclose >}}
{{< tdopen >}}
Optimality of Policies derived from the Cost-to-Go Function  
  
Policy Iteration  
  
Asynchronous Policy Iteration
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link fbd5a794-1a89-8177-6eb0-f5bd81308790 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
4
{{< tdclose >}}
{{< tdopen >}}
Average-Cost Problems  
  
Relationship with Discounted-Cost Problems  
  
Bellman's Equation  
  
Blackwell Optimality
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link 42e1a355-d7a7-4c79-abb0-bf9c959068e5 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
5
{{< tdclose >}}
{{< tdopen >}}
Average-Cost Problems  
  
Computational Methods
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link afcd934c-2ae9-e684-b68a-0022c692b9b0 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
6
{{< tdclose >}}
{{< tdopen >}}
Application of Value Iteration to Optimization of Multiclass Queueing Networks  
  
Introduction to Simulation-based Methods Real-Time Value Iteration
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link e15b4728-f0c8-8f22-f4d5-1d6166a2fb37 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
7
{{< tdclose >}}
{{< tdopen >}}
Q-Learning  
  
Stochastic Approximations
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link 712e86c6-5716-18f7-3c08-5f8da5a897cd "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
8
{{< tdclose >}}
{{< tdopen >}}
Stochastic Approximations: Lyapunov Function Analysis  
  
The ODE Method  
  
Convergence of Q-Learning
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link dbd61bc2-1861-760c-0e7b-7a3fff4d6120 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
9
{{< tdclose >}}
{{< tdopen >}}
Exploration versus Exploitation: The Complexity of Reinforcement Learning
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link 6be403af-d3e9-dabd-97dc-6d869cc11d52 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
10
{{< tdclose >}}
{{< tdopen >}}
Introduction to Value Function Approximation  
  
Curse of Dimensionality  
  
Approximation Architectures
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link 3154a1a3-93c8-70d0-128e-5a3e62579a8b "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
11
{{< tdclose >}}
{{< tdopen >}}
Model Selection and Complexity
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link 2e76683e-f20d-0e9e-31a5-74a37400d38c "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
12
{{< tdclose >}}
{{< tdopen >}}
Introduction to Value Function Approximation Algorithms  
  
Performance Bounds
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link b068165d-1535-e8c4-0105-ef047b24d253 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
13
{{< tdclose >}}
{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link 0269827b-2577-e18a-7e86-83b185c1737d "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
14
{{< tdclose >}}
{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation (cont.)
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link 994ed38f-40bc-232a-3ce4-908e9fa070e9 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
15
{{< tdclose >}}
{{< tdopen >}}
Temporal-Difference Learning with Value Function Approximation (cont.)  
  
Optimal Stopping Problems  
  
General Control Problems
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link 69b2f97c-7cbe-11f7-f637-788eac36dbb5 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
16
{{< tdclose >}}
{{< tdopen >}}
Approximate Linear Programming
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link 5aae8384-2c21-491b-0574-e8d334ccec24 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
17
{{< tdclose >}}
{{< tdopen >}}
Approximate Linear Programming (cont.)
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link b8d8cfcd-de14-f221-80a9-0047a5d7f101 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
18
{{< tdclose >}}
{{< tdopen >}}
Efficient Solutions for Approximate Linear Programming
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link 6a22c321-105b-b6fc-c9e5-230781613a5b "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
19
{{< tdclose >}}
{{< tdopen >}}
Efficient Solutions for Approximate Linear Programming: Factored MDPs
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link c578943c-b9b5-ba63-e397-6e01de52c61e "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
20
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link a14c2731-96b9-da94-3629-3adf3050f264 "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
21
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods (cont.)
{{< tdclose >}}
{{< tdopen >}}
({{% resource_link 00c633f0-3396-bdde-03aa-31a35491abcb "PDF" %}})
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
22
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods for POMDPs  
  
Application: Call Admission Control  
  
Actor-Critic Methods
{{< tdclose >}}
{{< tdopen >}}
 
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
23
{{< tdclose >}}
{{< tdopen >}}
Approximate POMDP Compression
{{< tdclose >}}
{{< tdopen >}}
 
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
24
{{< tdclose >}}
{{< tdopen >}}
Policy Search Methods: PEGASUS  
  
Application: Helicopter Control
{{< tdclose >}}
{{< tdopen >}}
 
{{< tdclose >}}

{{< trclose >}}

{{< tableclose >}}